import re
import torch
from collections import Counter, defaultdict

class BPETokenizer:
    def __init__(self, special_token=None):
        self.vocab = {}
        self.merges = []
        self.token_to_id = {}
        self.id_to_token = {}
        if special_token is None:
            self.special_token = ['<pad>', '<sos>','<eos>','<unk>']
        else:
            self.special_token = special_token

    def get_stats(self, vocab):
        pairs = defaultdict(int) # 23ê°œê°€ ë‚˜ì˜¤ëŠ” ê·¼ê±°ëŠ”??? 46ìŒì ˆì˜ ì ˆë°˜ìˆ˜ì¹˜ë‹¤.
        for word, freq in vocab.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pairs[(symbols[i], symbols[i + 1])] += freq
        return pairs

    def merge_vocab(self, pair, v_in):
        bigram = re.escape(' '.join(pair))
        # (?<!\S) : ì™¼ìª½ì— ê³µë°±ì´ ì•„ë‹Œ ë¬¸ì(=Non-space) ê°€ ì—†ì–´ì•¼ í•¨
        # (?!\S)  : ì˜¤ë¥¸ìª½ì— ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìê°€ ì—†ì–´ì•¼ í•¨
        pattern = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        v_out = {}
        for word in v_in:
            # pair ì— í•´ë‹¹ë˜ëŠ” v_inì€ ê³µë°±ì„ ì œê±°í•´ì„œ ì½ëŠ”ë‹¤.
            w_out = pattern.sub(''.join(pair), word)
            v_out[w_out] = v_in[word]
            # Dictionaryë¼ì„œ ê·¸ëŸ°ê°€?
        return v_out

    def train(self, corpus, num_merges=100):
        vocab = Counter(corpus.split())
        self.vocab = {' '.join(word) + ' </w>': freq for word, freq in vocab.items()}
        #tokens = Counter([' '.join(word) + ' </w>' for word in corpus])
        #ë°›ì€ ë‹¨ì–´ë°°ì—´ì„ ì „ë¶€ ê³µë°±ìœ¼ë¡œ ìª¼ê°œê³ , ë‹¨ì–´ì˜ ëì— </w>ë¥¼ í•˜ëŠ” ê²ƒ, ê·¸ë¦¬ê³  ë¹ˆë„ìˆ˜ ì…ë ¥
        for i in range(num_merges):
            pairs = self.get_stats(self.vocab)
            if not pairs:
                break
            best = max(pairs, key=pairs.get) #ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ í•©ì³ì„œ, ë‹¤ì‹œ ë‹¨ì–´ë°˜í™˜
            self.vocab = self.merge_vocab(best, self.vocab)
            self.merges.append(best) # self.mergeì˜ ElementëŠ” ë§ˆì¹˜, ì…ë ¥,ì˜ˆì¸¡ ê´€ê³„ê°™ë‹¤.

        tokens = set()
        for word in self.vocab.keys(): # KeyëŠ” ë‹¨ì–´ë“¤, ValuesëŠ” ë¹ˆë„
            tokens.update(word.split())
        #self.token_to_id = {token: idx for idx, token in enumerate(sorted(tokens))}
        #self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}
        idx = 0
        # ë‹¤ì‹œê¸ˆ tokensë¡œ ì¸í•´, vocabì‚¬ì „ê³¼ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤(ê²°ê³¼ë§Œ ë³´ê³ ëŠ” ìœ ì‚¬ ë»˜ì§“ì¸ë°?)

        for token in self.special_token: # ìŠ¤í˜ì…œ í† í°ì€ 4ê°œë‹¤.
            # í˜„ì¬ê¹Œì§€ ë‘˜ì€ ë¹ˆ ìƒíƒœì˜€ë‹¤.
            self.token_to_id[token] = idx
            self.id_to_token[idx] = token
            idx += 1 # idxëŠ” 3(0,1,2,3)ì—ì„œ ëë‚ ê²ƒì´ë‹¤.

        for token in sorted(tokens):
            if token not in self.token_to_id:
                self.token_to_id[token] = idx
                self.id_to_token[idx] = token
                idx += 1
        #í•™ìŠµì„ ëìœ¼ë¡œ ì–»ëŠ”ê²ƒì€, token_to_id, id_to_tokenì´ë‹¤.

    def encode_word(self, word):
        word = list(word) + ["</w>"] # ë‹¨ì–´ë¥¼ ì „ë¶€ ìŒì ˆë¡œ ìë¥´ê³ , ë‹¨ì–´ë¼ë¦¬ëŠ” </w>êµ¬ë¶„ìë¥¼ ì¤€ë‹¤.
        while True:
            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]
            pair_ranks = {pair: i for i, pair in enumerate(self.merges)}
            ranked = [(pair, pair_ranks[pair]) for pair in pairs if pair in pair_ranks]
            if not ranked:
                break
            best_pair = min(ranked, key=lambda x: x[1])[0]
            new_word = []
            i = 0
            while i < len(word):
                if i < len(word) - 1 and word[i] == best_pair[0] and word[i + 1] == best_pair[1]:
                    new_word.append(''.join(best_pair))
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = new_word
        return word

    # ì—¬ê¸°ì„œë¶€í„°ëŠ” í•™ìŠµìœ¼ë¡œ ì–»ì€ token_to_id, id_to_tokenì„ ì‚¬ìš©í•´ì„œ, ë³¸ê²© Tokenizerë¥¼ ì§„í–‰í•œë‹¤.
    def encode_sentence(self, sentence):
        encoded_sentence = [self.token_to_id['<sos>']]
        for word in sentence.strip().split():
            tokens = self.encode_word(word)
            encoded_sentence.extend([self.token_to_id.get(token, self.token_to_id["<unk>"]) for token in tokens])
        encoded_sentence.append(self.token_to_id['<eos>'])
        return encoded_sentence

    def decode_sentence(self, token_ids):
        tokens = [self.id_to_token.get(i,"<unk>") for i in token_ids]
        tokens = [t for t in tokens if t not in ["<sos>","<eos>","<pad>"]]
        sentence = ' '.join(tokens).replace("</w>","").strip()
        return sentence

    def encode_batch(self, sentences):
        return [self.encode_sentence(s) for s in sentences]

    def pad_sequences(self, sequences, max_len=None):
        pad_id = self.token_to_id["<pad>"]
        if max_len is None:
            max_len = max(len(seq) for seq in sequences)

        padded = []

        for seq in sequences:
            if len(seq) < max_len:
                seq = seq + [pad_id] * (max_len - len(seq))
            else:
                seq = seq[:max_len]
            padded.append(seq)
        return padded

    def batch_to_tensor(self, sentences, device="mps"):
        encoded = self.encode_batch(sentences)
        padded = self.pad_sequences(encoded)
        return torch.tensor(padded, dtype=torch.long, device=device)

corpus = "low lower lowest lowly lower newest wide wider"
tokenizer = BPETokenizer()
tokenizer.train(corpus, num_merges=30)

sentences = [
    "low lowest",
    "wide wider lowly",
    "newest low"
]

batch_tensor = tokenizer.batch_to_tensor(sentences)

print("ğŸ”¹ Batch Tensor Shape:", batch_tensor.shape)
print("ğŸ”¹ Batch Tensor:\n", batch_tensor)
