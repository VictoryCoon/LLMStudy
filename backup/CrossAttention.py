import torch
import torch.nn as nn
import torch.nn.functional as F

class CrossAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads # 4
        self.d_k = d_model // num_heads # 16 // 4 = 4
        self.d_model = d_model # 16

        self.W_q = nn.Linear(d_model, d_model) # 16X16
        self.W_k = nn.Linear(d_model, d_model) # 16X16
        self.W_v = nn.Linear(d_model, d_model) # 16X16
        self.W_o = nn.Linear(d_model, d_model) # 16X16

    def forward(self, Q, K, V):
        batch_size = Q.size(0)

        # [2,5,16] > Q.size(0) = [2,-1,4,4] > [2,4,(Auto-Calc),4](*2X5X16ì˜ ì¸ìˆ˜ë¶„í•´êµ¬ë‚˜ğŸ™ƒ) > [2,4,5,5]
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # [2,4,5,4]@[2,4,4,5] / sqrt(4) = 2 = [2,4,5,5]
        scores = (Q @ K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        # SoftmaxëŠ” ì¸ìê°’ xì™€ ì¸ìê°’ì—ëŒ€í•œ ì¸ë±ìŠ¤ië¥¼ ë°›ëŠ”ë‹¤
        # Softmax(x_i) = F.softmax(scores=x, dim=i)
        weights = F.softmax(scores, dim=-1) # softmaxëŠ” ì°¨ì›í•©ì„ 1ë¡œ ë§Œë“¤ë¿, í˜•íƒœëŠ” ìœ ì§€í•œë‹¤[2,4,5,5]
        # ì´ë¶€ë¶„ì´ ì‚¬ì‹¤ìƒ ê°€ì¥ ì¤‘ìš”í•œ ê²°ê³¼ê°’ìœ¼ë¡œ ë³´ì¸ë‹¤.
        output = weights @ V # [2,4,5,5] @ [2,4,5,5] = [2,4,5,5]

        # [2,4,5,5].transpose(1,2) > [2,5,4,5].contiguous()[ìœ ì§€] > [2,5,4,5].view[2,-1,16] > [2,5,16]
        # contiguous? : í…ì„œê°€ ë©”ëª¨ë¦¬ì— ì¸ì ‘í•˜ê²Œ(contiguous) ì €ì¥ë˜ë„ë¡ ë³´ì¥í•˜ëŠ” ê¸°ëŠ¥, ì•ˆì „í•œ ë©”ëª¨ë¦¬ì— ì˜ì˜ë¥¼ ë‘ì—ˆë‹¤ê³ í•˜ëŠ”ë°...
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # í˜•íƒœëŠ” ë‹¤ì‹œ [2,5,16]ìœ¼ë¡œ ëŒì•„ì™”ë‹¤.

        # Linear[16,16]ì€ ì„ í˜•ë³€í™˜ìœ¼ë¡œ, ì•ìœ¼ [2,5]ëŠ” ì˜í–¥ì„ ì£¼ì§€ì•ŠëŠ”ë‹¤. ë§ˆì§€ë§‰ ì°¨ì›ë§Œ d_modelë¡œ ì¶œë ¥(ê°™ìŒ)
        print(output.shape)
        return self.W_o(output) # [2,5,16]

batch_size = 2
seq_len = 5
d_model = 16
num_heads = 4

# Encoder Output
enc_output = torch.rand(batch_size, seq_len, d_model) # 2,5,16
# Decoder Hidden
dec_hidden = torch.rand(batch_size, seq_len, d_model) # 2,5,16

cross_attention = CrossAttention(d_model, num_heads) # 16, 4
# Q(dec_hidden), K(enc_output), V(enc_output)
result = cross_attention(dec_hidden, enc_output, enc_output)

print(f"ì…ë ¥(Decoder Hidden) : ${dec_hidden.shape}")
print(f"ì…ë ¥(Encoder Output) : ${enc_output.shape}")
print(f"ì¶œë ¥ : ${result.shape}")